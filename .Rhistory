dat <- data.frame(
x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE),
nosim), 1, var),
apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE),
nosim), 1, var),
apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE),
nosim), 1, var)
),
size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
library(ggplot2)
nosim <- 10000;
dat <- data.frame(
x = c(apply(matrix(rnorm(nosim * 10), nosim), 1, var),
apply(matrix(rnorm(nosim * 20), nosim), 1, var),
apply(matrix(rnorm(nosim * 30), nosim), 1, var)),
n = factor(rep(c("10", "20", "30"), c(nosim, nosim, nosim)))
)
ggplot(dat, aes(x = x, fill = n)) + geom_density(size = 2, alpha = .2) + geom_vline(xintercept = 1, size = 2)
dat <- data.frame(
x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE),
nosim), 1, var),
apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE),
nosim), 1, var),
apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE),
nosim), 1, var)
),
size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black")
g <- g + geom_vline(xintercept = 2.92, size = 2)
g + facet_grid(. ~ size)
install.packages('XML')
require(XML)
mydata.vectors <- character(0)
mydata.vectors
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
twitter_q <- URLencode('#prolife OR #prochoice')
twitter_q
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
twitter_url
page
c
round(c(var(x), var(x) / n, sd(x), sd(x) / sqrt(n)),2)
g <- ggplot(data = father.son, aes(x = sheight))
pkgs <- c('twitteR','ROAuth', 'httr', 'stringr', 'plyr')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
options(error=traceback)
options(show.error.locations = TRUE)
library(twitteR)
library(ROAuth)
library(httr)
api_key <- "********"
api_secret <- "********"
access_token <- "****************"
access_token_secret <- "********"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
api_key <- "xOnMCl2XZQf96M3S0E8ebGiEV"
api_secret <- "uSKcaXSklvu69wwLdymo0nZhgaKk6cHpiNTSGzas9Yg6hEq9NZ"
access_token <- "67433123-AZezkqDJswd011NJHjNTTDOwvCWyVfqmgFxd6MqsW"
access_token_secret <- "cZihVqQ2eKW5lnyS6IMaxh0gaQsLI54u1Euly82GUEGtw"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets_sanders <- searchTwitter('@BernieSanders', n=1500)
library(plyr)
feed_sanders = laply(tweets_sanders, function(t) t$getText())
getcwd()
getwd()
tweets_sanders
setwd("~/Dropbox/MSIM/INFX573/Course_Project/twitter-hate-speech-identification/Data Mining")
setwd("~/Dropbox/MSIM/INFX573/Course_Project/twitter-hate-speech-identification/Data Mining")
yay = scan('opinion-lexicon-English/positive-words.txt',
what='character', comment.char=';')
boo = scan('opinion-lexicon-English/negative-words.txt',
what='character', comment.char=';')
feed_sanders
yay
boo
help(searchTwitter)
bad_text = c(boo, 'wtf', 'epicfail', 'douchebag')
good_text = c(yay, 'upgrade', ':)', '#iVoted', 'voted')
score.sentiment = function(sentences, good_text, bad_text, .progress='none') {
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, good_text, bad_text) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence) # Tutorial messes up here. Needs 2 escapes
#to remove emojis
sentence <- iconv(sentence, 'UTF-8', 'ASCII')
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, good_text)
neg.matches = match(words, bad_text)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, good_text, bad_text, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
feelthabern <- score.sentiment(feed_sanders, good_text, bad_text, .progress='text')
feelthabern
plotdat <- plotdat[c("name", "score")]
plotdat
feelthabern
feelthabern[1]
feelthabern[0]
feelthabern[1]
feelthabern[2]
feelthabern[3]
feelthabern[1]
feelthabern[2][0]
feelthabern[2][1]
feelthabern[2][1][1]
feelthabern[2][
}
plotdat <- plotdat[!plotdat$score == 0, ]
qplot(factor(score), data=plotdat, geom="bar",
fill=factor(name),
xlab = "Sentiment Score")
plotdat <- plot(dat[c("name", "score")])
dat
dat[c("name", "score")]
plotdat
help(plotdat)
plotdat = feelthabern
plotdat <- plotdat[c("name", "score")]
qplot(factor(score), data=plotdat, geom="bar",
fill=factor(name),
xlab = "Sentiment Score")
feelthabern <- score.sentiment(feed_sanders, good_text, bad_text, .progress='text')
plotdat <- feelthabern[c("name", "score")]
plotdat <- plotdat[!plotdat$score == 0, ]
qplot(factor(score), data=plotdat, geom="bar",
xlab = "Sentiment Score")
help(laply)
class(bad_text)
bad_text
bad_text[0]
bad_text[1]
bad_text[15]
len(bad_text)
bad_text[15]
bad_text
feelthabern
feelthabern[0]
feelthabern[1]
str(feelthabern)
plotdat
str(feelthabern)
### Code from https://www.r-bloggers.com/how-to-use-r-to-scrape-tweets-super-tuesday-2016/
#---- head ----#
# Install needed packages if necessary
pkgs <- c('twitteR','ROAuth', 'httr', 'stringr', 'plyr')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
options(error=traceback)
options(show.error.locations = TRUE)
#---- /head ----#
library(twitteR)
library(ROAuth)
library(httr)
## MAKE AN ACCOUNT AND ENTER IN YOUR CREDENTIALS FROM HERE:
# http://docs.inboundnow.com/guide/create-twitter-application/
# Set API Keys
api_key <- "xOnMCl2XZQf96M3S0E8ebGiEV"
api_secret <- "uSKcaXSklvu69wwLdymo0nZhgaKk6cHpiNTSGzas9Yg6hEq9NZ"
access_token <- "67433123-AZezkqDJswd011NJHjNTTDOwvCWyVfqmgFxd6MqsW"
access_token_secret <- "cZihVqQ2eKW5lnyS6IMaxh0gaQsLI54u1Euly82GUEGtw"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
# You are in.
# Grab latest tweets
tweets_sanders <- searchTwitter('@BernieSanders', n=1500)
# Loop over tweets and extract text
library(plyr)
feed_sanders = laply(tweets_sanders, function(t) t$getText())
######## Set working directory
setwd("~/Dropbox/MSIM/INFX573/Course_Project/twitter-hate-speech-identification/Data Mining")
# Hu and Liu opinion lexicon 5000 words
# Unpack it
# Read in dictionary of positive and negative works
yay = scan('opinion-lexicon-English/positive-words.txt',
what='character', comment.char=';')
boo = scan('opinion-lexicon-English/negative-words.txt',
what='character', comment.char=';')
# Add a few twitter-specific negative phrases
bad_text = c(boo, 'wtf', 'epicfail', 'douchebag')
good_text = c(yay, 'upgrade', ':)', '#iVoted', 'voted')
score.sentiment = function(sentences, good_text, bad_text, .progress='none') {
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, good_text, bad_text) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence) # Tutorial messes up here. Needs 2 escapes
#to remove emojis
sentence <- iconv(sentence, 'UTF-8', 'ASCII')
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, good_text)
neg.matches = match(words, bad_text)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, good_text, bad_text, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
### This references a field not defined
# # Call the function and return a data frame
feelthabern <- score.sentiment(feed_sanders, good_text, bad_text, .progress='text')
# # Cut the text, just gets in the way
plotdat <- feelthabern[c("name", "score")]
# # Remove neutral values of 0
plotdat <- plotdat[!plotdat$score == 0, ]
#
# # Nice little quick plot
qplot(factor(score), data=plotdat, geom="bar",
xlab = "Sentiment Score")
tweet1 <- tweets_sanders[1]
tweet1
tweet1$getText()
View(feelthabern)
View(feelthabern)
View(dat)
View(dat)
View(plotdat)
View(plotdat)
twitter_sanders$x
colnames(twitter_sanders)
colnames(tweets_sanders)
tweets_sanders
tweets_sanders[1]
tweets_sanders[1][1]
class(tweets_sanders[1])
tweets_sanders$test
tweets_sanders$text
tweets_sanders[1]$text
tweets_sanders[1]['text']
tweets_sanders['text']
class(tweets_sanders)
tweets_sanders[1:10]
test(tweets_sanders)
test(tweets_sanders[1])
tweets_sanders$getText()
tweets_sanders[1]$getText()
class(tweets_sanders[1])
feed_sanders_screenName = laply(tweets_sanders, function(t) t$getScreenName())
feed_sanders_screenName
tweets_random <- searchTwitter( n=1500)
tweets_random <- searchTwitter( n=1500)
help(searchTwitter)
tweets_random <- searchTwitter('hashtag', n=1500)
feed_random <- laply(tweets_random, function(t) t$getText())
feed_random
random_tweetScore <- score.sentiment(feed_sanders, good_text, bad_text, .progress='text')
View(random_tweetScore)
View(random_tweetScore)
random_tweetScore <- score.sentiment(feed_random, good_text, bad_text, .progress='text')
help(searchTwitter)
tweets_random <- searchTwitter('hashtag', since = '2016-01-01', until = '2016-10-01' , n=1500)
tweets_random <- searchTwitter('hashtag', since = '2016-01-01' , n=1500)
setwd("~/Dropbox/MSIM/INFX573/Course_Project/twitter-hate-speech-identification")
pkgs <- c('twitteR','ROAuth', 'httr', 'stringr', 'plyr')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
options(error=traceback)
options(error=traceback)
options(show.error.locations = TRUE)
library(twitteR)
library(ROAuth)
library(httr)
api_key <- "xOnMCl2XZQf96M3S0E8ebGiEV"
api_secret <- "uSKcaXSklvu69wwLdymo0nZhgaKk6cHpiNTSGzas9Yg6hEq9NZ"
access_token <- "67433123-AZezkqDJswd011NJHjNTTDOwvCWyVfqmgFxd6MqsW"
access_token_secret <- "cZihVqQ2eKW5lnyS6IMaxh0gaQsLI54u1Euly82GUEGtw"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=10000)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=5000)
tweets_pre_election <- searchTwitter('hashtag', until = '2016-11-08' , n=5000)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=5000)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=3000)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=2000)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=1500)
api_key <- "xOnMCl2XZQf96M3S0E8ebGiEV"
api_secret <- "uSKcaXSklvu69wwLdymo0nZhgaKk6cHpiNTSGzas9Yg6hEq9NZ"
access_token <- "67433123-AZezkqDJswd011NJHjNTTDOwvCWyVfqmgFxd6MqsW"
access_token_secret <- "cZihVqQ2eKW5lnyS6IMaxh0gaQsLI54u1Euly82GUEGtw"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=1500)
tweets_post_election <- searchTwitter('hashtag', n=1500)
getwd()
setwd("/Users/amir/Dropbox/MSIM/INFX573/Course_Project/twitter-hate-speech-identification")
dataset_original = read.csv('/data/labeled_data.csv', sep = ",")
dataset_original = read.csv('data/labeled_data.csv', sep = ",")
pkgs <- c('tm','SnowballC')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
View(dataset_original)
View(dataset_original)
tweets_post_election <- searchTwitter('hashtag', since = '2016-11-08' , n=1500)
tweets_pre_election <- searchTwitter('hashtag', since = '2016-01-01', until = '2016-11-08' , n=1500)
help(getText)
tweets_post_election
class(tweets_post_election)
help(tweets_post_election)
feed_post_text <- laply(tweets_post_election, function(t) t$getText())
feed_post_screenname <- laply(tweets_post_election, function(t) t$getScreenName())
str(tweets_post_election)
feed_post_longitude <- laply(tweets_post_election, function(t) t$getLongitude())
feed_post_latitude <- laply(tweets_post_election, function(t) t$getLatitude())
post_text <- laply(tweets_post_election, function(t) t$getText())
post_screenname <- laply(tweets_post_election, function(t) t$getScreenName())
post_latitude <- laply(tweets_post_election, function(t) t$getLatitude())
post_longitude <- laply(tweets_post_election, function(t) t$getLongitude())
post_retweetcount <- laply(tweets_post_election, function(t) t$getRetweetCount())
tweetssc.df <- twListToDF(tweets_post_election)
View(tweetssc.df)
View(tweetssc.df)
tweets_post.df <- twListToDF(tweets_post_election)
dim(tweets_post.df)
str(tweets_post.df)
colnames(tweets_post.df)
date<-Sys.Date()
date
save(tweets_post.df, file =name)
save(tweets_post.df, file = 'tweets_post.csv')
write.csv(tweets_post.df, file = 'tweets_post.csv')
write.csv(tweets_post.df, file = 'tweets_post_election.csv')
colnames(tweets_post.df)
df <- setNames(tweets_post.df.frame(as.list(1:5)), LETTERS[1:5])
subset(tweets_post.df, select=c("text", "retweetCount"))
subset_tweets <- subset(tweets_post.df, select=c("text", "retweetCount", "retweeted", "created"))
write.csv(subset_tweets, file = 'tweets_post_election.csv')
write.csv(subset_tweets, file = 'tweets_post_election.csv')
corpus = VCorpus(VectorSource(dataset_original$tweet))
corpus = VCorpus(VectorSource(dataset_original$tweet))
subset(tweets_post.df, select=c("text", "retweetCount", "retweeted", "created"))
setwd("/Users/amir/Dropbox/MSIM/INFX573/Course_Project/twitter-hate-speech-identification")
corpus = VCorpus(VectorSource(dataset_original$tweet))
setwd("/Users/amir/Dropbox/MSIM/INFX573/Course_Project/twitter-hate-speech-identification")
dataset_original = read.csv('data/labeled_data.csv', sep = ",")
pkgs <- c('tm','SnowballC')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$tweet))
corpus
View(corpus)
VectorSource(dataset_original$tweet)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
dataset$hate_speech = dataset_original$hate_speech
View(dataset)
View(dataset)
dataset$hate_speech
dataset$hate_speech = factor(dataset$hate_speech, levels = c(0, 1, 2, 3))
dataset$hate_speech
pkgs <- c('tm','SnowballC', 'caTools')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
library(tm)
library(SnowballC)
library(caTools)
set.seed(1)
split = sample.split(dataset$hate_speech, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(training_set)
View(training_set)
View(training_set)
dim(training_set)
dim(test_set)
pkgs <- c('tm','SnowballC', 'caTools', 'randomForest')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
library(tm)
library(SnowballC)
library(caTools)
library(randomForest)
training_set[-995]
dim(training_set[-995])
dim(training_set)
dim(training_set[-996])
training_set[-996]
dim(training_set)
dim(training_set)[1]
dim(training_set)[2]
ncol(training_set)
classifier = randomForest(x = training_set[-ncol(training_set)],
y = training_set$hate_speech,
ntree = 10)
y_pred = predict(classifier, newdata = test_set[-numCol)
y_pred = predict(classifier, newdata = test_set[-numCol])
test_set[-numCol]
numCol = ncol(training_set)
y_pred = predict(classifier, newdata = test_set[-numCol])
cm = table(test_set[, numCol], y_pred)
cm
library (ROCR)
pkgs <- c('tm','SnowballC', 'caTools', 'randomForest', 'ROCR')
for (pkg in pkgs) {
if(!(pkg %in% rownames(installed.packages()))) {
install.packages(pkg)
}
}
library (ROCR)
predictions <- prediction(y_pred ,test_set[, numCol])
predictions <- prediction(y_pred ,test_set[, numCol])
y_pred
test_set[, numCol]
predictions <- prediction(y_pred ,test_set[, numCol])
help(table)
cm = table(test_set[, numCol], y_pred)
cm
n = sum(cm) # number of instances
n
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class
diag
rowsums = apply(cm, 1, sum) # number of instances per class
colsums = apply(cm, 2, sum) # number of predictions per class
p = rowsums / n # distribution of instances over the actual classes
q = colsums / n # distribution of instances over the predicted classes
accuracy = sum(diag) / n
accuracy
precision = diag / colsums
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
accuracy = sum(diag) / n
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
mod <- glm(default~balance + income, data=Default, family="binomial")
summary(
)
load(Default)
library(Default)
View(dataset)
View(dataset)
